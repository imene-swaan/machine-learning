{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial features :\n",
    "from sklearn import preprocessing\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(\n",
    " degree=2,\n",
    " interaction_only=False,\n",
    " include_bias=False)\n",
    "\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(\n",
    " poly_feats,\n",
    " columns=[f\"f_{i}\" for i in range(1, num_feats + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying log to reduce variance\n",
    "df.f_3.apply(lambda x: np.log(1 + x)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10. ,  6. , 13. ,  8.5,  7.5, 14. ],\n",
       "       [ 2. , 11. ,  5. ,  9. ,  7. , 11. ],\n",
       "       [12. ,  3. ,  7. ,  5. , 14. , 10. ],\n",
       "       [ 4. , 14. , 13. ,  6. ,  5. ,  6. ],\n",
       "       [ 6.5,  5. ,  2. ,  2. ,  3. ,  3. ],\n",
       "       [ 7. , 12. ,  2. ,  3. ,  7. ,  1. ],\n",
       "       [10. , 12. ,  2. ,  3. ,  4. ,  2. ],\n",
       "       [14. ,  4. ,  1. ,  1. , 12. ,  5. ],\n",
       "       [13. , 10. , 14. ,  8. ,  1. , 11. ],\n",
       "       [ 6. , 12. ,  2. ,  2. ,  2. ,  2. ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import impute\n",
    "\n",
    "# create a random numpy array with 10 samples and 6 features and values ranging from 1 to 15\n",
    "X = np.random.randint(1, 15, (10, 6))\n",
    "\n",
    "# convert the array to float\n",
    "X = X.astype(float)\n",
    "\n",
    "# randomly assign 10 elements to NaN (missing)\n",
    "X.ravel()[np.random.choice(X.size, 10, replace=False)] = np.nan\n",
    "\n",
    "# use 3 nearest neighbours to fill na values\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=2)\n",
    "knn_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate both packages to use\n",
    "encoder = OrdinalEncoder()\n",
    "imputer = KNN()\n",
    "# create a list of categorical columns to iterate over\n",
    "cat_cols = ['embarked','class1','deck1','who','embark_town','sex','adult_male','alive','alone']\n",
    "\n",
    "def encode(data):\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "#create a for loop to iterate through each column in the data\n",
    "for columns in cat_cols:\n",
    "    encode(impute_data[columns])\n",
    "    \n",
    "# impute data and convert \n",
    "encode_data = pd.DataFrame(np.round(imputer.fit_transform(impute_data)),columns = impute_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data = ...\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "transformed_data = var_thresh.fit_transform(data)\n",
    "# transformed data will have all columns with variance less\n",
    "# than 0.1 removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate\n",
    "feature selection is nothing but a scoring of each feature against a given target.\n",
    "Mutual information, ANOVA F-test and chi2 are some of the most popular\n",
    "methods for univariate feature selection. There are two ways of using these in scikitlearn.\n",
    "- SelectKBest: It keeps the top-k scoring features\n",
    "- SelectPercentile: It keeps the top features which are in a percentage\n",
    "specified by the user\n",
    "\n",
    "It must be noted that you can use chi2 only for data which is non-negative in nature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "class UnivariateFeatureSelction:\n",
    "    def __init__(self, n_features, problem_type, scoring):\n",
    "     \"\"\"\n",
    "     Custom univariate feature selection wrapper on\n",
    "     different univariate feature selection models from\n",
    "     scikit-learn.\n",
    "     :param n_features: SelectPercentile if float else SelectKBest\n",
    "     :param problem_type: classification or regression\n",
    "     :param scoring: scoring function, string\n",
    "     \"\"\"\n",
    "     # for a given problem type, there are only\n",
    "     # a few valid scoring methods\n",
    "     # you can extend this with your own custom\n",
    "     # methods if you wish\n",
    "    if problem_type == \"classification\":\n",
    "            valid_scoring = { \"f_classif\": f_classif, \"chi2\": chi2,\"mutual_info_classif\": mutual_info_classif }\n",
    "    else:\n",
    "        valid_scoring = { \"f_regression\": f_regression, \"mutual_info_regression\": mutual_info_regression }\n",
    "\n",
    "    # raise exception if we do not have a valid scoring method\n",
    "    if scoring not in valid_scoring:\n",
    "        raise Exception(\"Invalid scoring function\")\n",
    "\n",
    "     # if n_features is int, we use selectkbest\n",
    "     # if n_features is float, we use selectpercentile\n",
    "     # please note that it is int in both cases in sklearn\n",
    "    if isinstance(n_features, int):\n",
    "        self.selection = SelectKBest( valid_scoring[scoring], k=n_features)\n",
    "    \n",
    "    elif isinstance(n_features, float):\n",
    "        self.selection = SelectPercentile(valid_scoring[scoring], percentile=int(n_features * 100))\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Invalid type of feature\")\n",
    "\n",
    "     # same fit function\n",
    "     def fit(self, X, y):\n",
    "            return self.selection.fit(X, y)\n",
    "\n",
    "     # same transform function\n",
    "     def transform(self, X):\n",
    "            return self.selection.transform(X)\n",
    "\n",
    "     # same fit_transform function\n",
    "     def fit_transform(self, X, y):\n",
    "            return self.selection.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufs = UnivariateFeatureSelction(\n",
    " n_features=0.1,\n",
    " problem_type=\"regression\",\n",
    " scoring=\"f_regression\"\n",
    ")\n",
    "ufs.fit(X, y)\n",
    "X_transformed = ufs.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "# initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# initialize RFE\n",
    "rfe = RFE(estimator=model, n_features_to_select=3)\n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# get the transformed data with selected columns\n",
    "X_transformed = rfe.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### greedy feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy.py\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class GreedyFeatureSelection:\n",
    "    \"\"\"\n",
    "    A simple and custom class for greedy feature selection.\n",
    "    You will need to modify it quite a bit to make it suitable\n",
    "    for your dataset.\n",
    "    \"\"\"\n",
    "    def evaluate_score(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function evaluates model on data and returns\n",
    "        Area Under ROC Curve (AUC)\n",
    "        NOTE: We fit the data and calculate AUC on same data.\n",
    "        WE ARE OVERFITTING HERE.\n",
    "        But this is also a way to achieve greedy selection.\n",
    "        k-fold will take k times longer.\n",
    "        If you want to implement it in really correct way,\n",
    "        calculate OOF AUC and return mean AUC over k folds.\n",
    "        This requires only a few lines of change and has been\n",
    "        shown a few times in this book.\n",
    "        :param X: training data\n",
    "        :param y: targets\n",
    "        :return: overfitted area under the roc curve\n",
    "        \"\"\"\n",
    "        \n",
    "         # fit the logistic regression model,\n",
    "         # and calculate AUC on same data\n",
    "         # again: BEWARE\n",
    "         # you can choose any model that suits your data\n",
    "        \n",
    "        model = linear_model.LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        predictions = model.predict_proba(X)[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, predictions)\n",
    "        return auc\n",
    "\n",
    "    def _feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "         This function does the actual greedy selection\n",
    "         :param X: data, numpy array\n",
    "         :param y: targets, numpy array\n",
    "         :return: (best scores, best features)\n",
    "        \"\"\"\n",
    "        # initialize good features list\n",
    "        # and best scores to keep track of both\n",
    "        good_features = []\n",
    "        best_scores = []\n",
    "\n",
    "        # calculate the number of features\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        # infinite loop\n",
    "        while True:\n",
    "            # initialize best feature and score of this loop\n",
    "            this_feature = None\n",
    "            best_score = 0\n",
    "            # loop over all features\n",
    "            for feature in range(num_features):\n",
    "                # if feature is already in good features, skip this for loop\n",
    "                if feature in good_features:\n",
    "                    continue\n",
    "                # selected features are all good features till now and current feature\n",
    "                selected_features = good_features + [feature]\n",
    "                # remove all other features from data\n",
    "                xtrain = X[:, selected_features]\n",
    "                # calculate the score, in our case, AUC\n",
    "                score = self.evaluate_score(xtrain, y)\n",
    "                # if score is greater than the best score\n",
    "                # of this loop, change best score and best feature\n",
    "                if score > best_score:\n",
    "                    this_feature = feature\n",
    "                    best_score = score\n",
    "            # if we have selected a feature, add it to the good feature list and update best scores list\n",
    "            if this_feature != None:\n",
    "                good_features.append(this_feature)\n",
    "                best_scores.append(best_score)\n",
    "            \n",
    "            # if we didnt improve during the last two rounds, exit the while loop\n",
    "            if len(best_scores) > 2:\n",
    "                if best_scores[-1] < best_scores[-2]:\n",
    "                    break\n",
    "        # return best scores and good features\n",
    "        # why do we remove the last data point?\n",
    "        return best_scores[:-1], good_features[:-1]\n",
    "        \n",
    "    def __call__(self, X, y):\n",
    "        \"\"\"\n",
    "        Call function will call the class on a set of arguments\n",
    "        \"\"\"\n",
    "        # select features, return scores and selected indices\n",
    "        scores, features = self._feature_selection(X, y)\n",
    "        # transform data with selected features\n",
    "        return X[:, features], scores\n",
    "if __name__ == \"__main__\":\n",
    "    # generate binary classification data\n",
    "    X, y = make_classification(n_samples=1000, n_features=100)\n",
    "    # transform data by greedy feature selection\n",
    "    X_transformed, scores = GreedyFeatureSelection()(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy feature selection implemented the way returns scores and a list of\n",
    "feature indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importance in features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# fetch a regression dataset in diabetes data we predict diabetes progression after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "\n",
    "# initialize the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "importances = model.feature_importances_\n",
    "idxs = np.argsort(importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(idxs)), importances[idxs], align='center')\n",
    "plt.yticks(range(len(idxs)), [col_names[i] for i in idxs])\n",
    "plt.xlabel('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "\n",
    "\n",
    "# initialize the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# select from the model\n",
    "sfm = SelectFromModel(estimator=model)\n",
    "X_transformed = sfm.fit_transform(X, y)\n",
    "\n",
    "# see which features were selected\n",
    "support = sfm.get_support()\n",
    "\n",
    "# get feature names\n",
    "print([x for x, y in zip(col_names, support) if y == True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
